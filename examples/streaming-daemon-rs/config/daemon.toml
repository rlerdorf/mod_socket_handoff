# Streaming Daemon Configuration
#
# All settings can be overridden with environment variables.
# Format: DAEMON_<UPPER_SNAKE_KEY> (e.g., DAEMON_SOCKET_PATH, DAEMON_MAX_CONNECTIONS)

[server]
# Unix socket path for receiving handoffs from Apache
# Must be under Apache's SocketHandoffAllowedPrefix
# Note: When using the systemd service, this is overridden by DAEMON_SOCKET_PATH
# environment variable to use RuntimeDirectory (/run/streaming-daemon-rs/)
socket_path = "/run/streaming-daemon-rs/daemon.sock"

# Socket file permissions (octal)
# 0o660 allows only the owning user and group to connect (recommended)
# Use 0o666 only in development or single-user environments
socket_mode = "0o660"

# Maximum concurrent connections
# Aggressive setting for high-scale production
max_connections = 100000

# Timeout for receiving handoff from Apache (seconds)
handoff_timeout_secs = 5

# Timeout for individual writes to client (seconds)
# Resets after each successful write, not total stream time
write_timeout_secs = 30

# Graceful shutdown timeout (seconds)
# Should be long enough for LLM streams to complete
shutdown_timeout_secs = 120

# Buffer size for receiving handoff data (bytes)
# Should accommodate LLM prompts; Apache's LimitRequestFieldSize limits headers
handoff_buffer_size = 65536

# Timeout for backend stream creation (seconds)
# Prevents indefinite blocking when the upstream API hangs
backend_timeout_secs = 30

[backend]
# Backend provider: "mock" or "openai"
# Use "mock" for testing/demos
provider = "mock"

# Default model to use (if not specified in handoff data)
default_model = "gpt-4o"

# API timeout (seconds)
timeout_secs = 120

[backend.openai]
# OpenAI API key (can also be set via OPENAI_API_KEY env var)
# api_key = "sk-..."

# OpenAI API base URL
api_base = "https://api.openai.com/v1"

# Unix socket path for API connections (HTTP/1.1 mode only)
# Eliminates ephemeral port exhaustion under high concurrency
# api_socket = "/var/run/openai-proxy.sock"

# Maximum idle connections per host in the HTTP pool
pool_max_idle_per_host = 100

# Skip TLS certificate verification (for testing with self-signed certs only)
# WARNING: Never enable in production
insecure_ssl = false

[backend.openai.http2]
# Enable HTTP/2 multiplexing for upstream API connections (default: true)
# ~100 concurrent streams share a single TCP connection, reducing overhead
enabled = true

# Flow control window sizes
initial_stream_window_kb = 1024    # Per-stream window (1 MB)
initial_connection_window_kb = 10240  # Per-connection window (10 MB)

# Adaptive flow control adjusts windows based on actual throughput
adaptive_window = true

# HTTP/2 keep-alive PING frames (0 = disabled)
keep_alive_interval_secs = 10
keep_alive_timeout_secs = 20

[metrics]
# Enable Prometheus metrics endpoint
enabled = true

# Listen address for metrics server
listen_addr = "127.0.0.1:9090"

[logging]
# Log level: trace, debug, info, warn, error
level = "info"

# Log format: pretty or json
format = "pretty"
