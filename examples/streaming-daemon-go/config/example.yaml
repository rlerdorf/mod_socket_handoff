# Example configuration for streaming-daemon
# Copy to local.yaml and customize (local.yaml is gitignored)

server:
  socket_path: /var/run/streaming-daemon.sock
  socket_mode: 0660
  max_connections: 50000
  max_stream_duration_ms: 300000  # Max per-stream duration in ms (0 = no timeout, default: 300000 = 5 min)
  # pprof_addr: localhost:6060  # Uncomment to enable profiling
  # memlimit: 768MiB            # Soft memory limit (e.g. 512MiB, 1GiB); empty = no limit
  # gc_percent: 100             # GOGC value; 0 = use Go default (100)

backend:
  # Available providers: langgraph, mock, openai, typing
  provider: openai
  default_model: gpt-4o-mini

  # OpenAI-compatible API configuration
  openai:
    # SECURITY: Never store API keys in config files for production.
    # Use OPENAI_API_KEY environment variable instead (recommended).
    # api_key: sk-...
    api_base: https://api.openai.com/v1
    # api_socket: /var/run/openai-proxy.sock  # Unix socket for high concurrency
    http2_enabled: true
    insecure_ssl: false

  # LangGraph Platform API configuration
  langgraph:
    # SECURITY: Never store API keys in config files for production.
    # Use LANGGRAPH_API_KEY environment variable instead (recommended).
    # api_key: lgk_...
    api_base: https://api.langchain.com/v1
    # api_socket: /var/run/langgraph-proxy.sock  # Unix socket for high concurrency
    assistant_id: agent
    stream_mode: messages-tuple
    http2_enabled: true
    insecure_ssl: false

  # Mock backend configuration
  mock:
    message_delay_ms: 50

  # Typing backend configuration
  typing: {}

metrics:
  enabled: true
  listen_addr: 127.0.0.1:9090

logging:
  level: info    # debug, info, warn, error
  format: text   # text, json
